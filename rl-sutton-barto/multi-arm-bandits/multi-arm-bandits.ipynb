{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Action-Value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by looking more closely at some simple methods for estimating the\n",
    "values of actions and for using the estimates to make action selection decisions.\n",
    "In this chapter, we denote the true (actual) value of action $a$ as $q(a)$, and the\n",
    "estimated value on the $t$th time step as $Q_{t}(a)$. Recall that the true value of an\n",
    "action is the mean reward received when that action is selected. One natural\n",
    "way to estimate this is by averaging the rewards actually received when the\n",
    "action was selected. In other words, if by the $t$th time step action $a$ has been\n",
    "chosen $N_{t}(a)$ times prior to $t$, yielding rewards $R_{1}, R_{2}, . . . ,R_{N_{t}(a)}$\n",
    ", then its value\n",
    "is estimated to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_{t}(a) = \\frac{R_{1}+R_{2}+...+R_{N_{t}(a)}}{N_{t}(a)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_average(R):\n",
    "    return sum(R)/len(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $N_{t}(a) = 0$, then we define $Q_{t}(a)$ instead as some default value, such as\n",
    "$Q_{1}(a) = 0$. As $N_{t}(a) → \\infty$, by the law of large numbers, $Q_{t}(a)$ converges\n",
    "to $q(a)$. We call this the sample-average method for estimating action values\n",
    "because each estimate is a simple average of the sample of relevant rewards.\n",
    "Of course this is just one way to estimate action values, and not necessarily\n",
    "the best one. Nevertheless, for now let us stay with this simple estimation\n",
    "method and turn to the question of how the estimates might be used to select\n",
    "actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step $t$ one of the greedy actions, $A^{∗}_{t}$, for which $Q_{t}(A^{∗}_{t})= max_{a}Q_{t}(a)$. This greedy action\n",
    "selection method can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^{*}_{t} = argmax_{a}Q_{t}(a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def greedy_policy(Q):\n",
    "    #returns the greedy action according to Q\n",
    "    #Q: a dictionary, where keys are action and values are the rewards we've collected for action a so far. \n",
    "    #We record a list of best actions, so that ties are broken randomly\n",
    "    best_actions = [None]\n",
    "    best_value = float('-inf')\n",
    "    for a in Q.keys(): #for each action in Q\n",
    "        av_reward = sample_average(Q[a])\n",
    "        if av_reward > best_value: #take on action if it's at least as good\n",
    "            best_actions = [a]\n",
    "            best_value = av_reward\n",
    "        elif av_reward == best_value: #add action to list\n",
    "            best_actions.append(a)\n",
    "    \n",
    "    return(random.choice(best_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $argmax_{a}$ denotes the value of $a$ at which the expression that follows\n",
    "is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\\epsilon$, instead to select randomly from amongst all the actions with equal probability independently of the action value estimates. We call methods using this near-greedy action selection rule $\\epsilon$-greedy methods. An advantage of these methods is that, in the limit as the number of plays increases, every action will be sampled an infinite number of times, guaranteeing that $N_{t}(a) → \\infty$ for all $a$, and thus ensuring that all\n",
    "the $Q_{t}(a)$ converge to $q(a)$. This of course implies that the probability of selecting the optimal action converges to greater than 1 − $\\epsilon$, that is, to near\n",
    "certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def epsilon_greedy_policy(Q,epsilon):\n",
    "    #returns the epsilon-greedy action according to Q\n",
    "    population = [greedy_policy(Q),random.choice(list(Q.keys()))] #[greedy,random]\n",
    "    weights = [1-epsilon,epsilon]\n",
    "    return random.choices(population,weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roughly assess the relative effectiveness of the greedy and $\\epsilon$-greedy methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated n-armed bandit tasks with $n = 10$. For each bandit, the action values, $q(a)$, $a = 1, . . . , 10$, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. On $t$th time step with a given bandit, the actual reward $R_{t}$ was the $q(A_{t})$ for the bandit (where $A_{t}$ was the action selected) plus a normally distributed noise term that was mean 0 and variance 1. Averaging over bandits, we can plot the performance and behavior of various methods as they improve with experience over 1000 steps. We call this suite of test tasks the 10-armed testbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def ten_armed_testbed(num_arms=10,num_bandits=1000,steps=500,epsilons=[0.1,0.01,0.0]):\n",
    "    #implementing the ten armed testbed for epsilon greedy policy. For each setting of epsilon, we do num_bandit experiments an average over them. \n",
    "    av_rewards = {} #dictionary, keys are epsilon, value will be list of average reward across time steps\n",
    "    opt_actions = {} #dictionary, keys are epsilon, value will be list of percent of actions which were optimal across time steps\n",
    "    \n",
    "    for epsilon in epsilons: #for each epsilon\n",
    "        av_reward = [] #will contain a list of rewards at each time step for each bandit\n",
    "        opt_action = [] #will contain whether action at each time step for each bandit was optimal\n",
    "        for bandit in tqdm(range(num_bandits)): \n",
    "            rewards = [] #will collect the rewards\n",
    "            actions = [] #will collect whether action optimal or not\n",
    "            #Rewards collected from each action, which we call Q(a) (although we need to average it in order for it to be Q(a) as described above)\n",
    "            Q = {a:[0] for a in range(num_arms)} #we initialize the first return of all actions to 0 so greedy policy selection is happy\n",
    "            \n",
    "            #generate the bandit, q(a)\n",
    "            q = np.random.normal(0,1,num_arms)\n",
    "            #figure out optimal action\n",
    "            max_v = max(q) #max value\n",
    "            max_a = [i for i,j in enumerate(q) if j == max_v][0] #get argmax action\n",
    "            #print(q)\n",
    "            #print(\"the best q value is:\",max_v, \"best action is \", max_a)\n",
    "            for step in range(steps): #simulate the bandit\n",
    "                action = epsilon_greedy_policy(Q,epsilon)\n",
    "                reward = q[action] + np.random.normal(0,1) #add noise to the reward\n",
    "                #append reward to rewards\n",
    "                rewards.append(reward)\n",
    "                #append reward to estimation\n",
    "                Q[action].append(reward)\n",
    "                #check if action was optimal\n",
    "                actions.append(1 if action == max_a else 0)\n",
    "                #print(\"the q value for best is:\",sample_average(Q[max_a]),\"and current action is \",action)\n",
    "            av_reward.append(rewards)\n",
    "            opt_action.append(actions)\n",
    "        #average the rewards and optimal actions across all the bandits for each time step. python magic\n",
    "        epsilon_av_reward = list(map(sample_average, zip(*av_reward)))\n",
    "        epsilon_opt_action = list(map(sample_average, zip(*opt_action)))\n",
    "        av_rewards[epsilon] = epsilon_av_reward\n",
    "        opt_actions[epsilon] = epsilon_opt_action\n",
    "        \n",
    "    #Visualize the results\n",
    "    plt.plot(range(len(av_rewards[0.1])),av_rewards[0.1],c='black')\n",
    "    plt.plot(range(len(av_rewards[0.01])),av_rewards[0.01],c='red')\n",
    "    plt.plot(range(len(av_rewards[0.0])),av_rewards[0.0],c='green')\n",
    "    plt.show()\n",
    "    plt.plot(range(len(opt_actions[0.1])),opt_actions[0.1],c='black')\n",
    "    plt.plot(range(len(opt_actions[0.01])),opt_actions[0.01],c='red')\n",
    "    plt.plot(range(len(opt_actions[0.0])),opt_actions[0.0],c='green')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 279/1000 [00:09<00:25, 28.01it/s]"
     ]
    }
   ],
   "source": [
    "ten_armed_testbed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture of average reward\n",
    "<img src=\"./av_reward.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture of optimal action percentage\n",
    "<img src=\"./optimal_action.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
