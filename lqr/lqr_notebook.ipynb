{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a jupyter notebook version of an implementation of a Linear-Quadratic Regulator (LQR) in python for solving the OpenAI Gym environment of CartPole. This notebook will only give a brief-overview of LQR, and if you are interested in digging deeper into control theory, I highly suggest watching Steve Brunton's \"Control Bootcamp\" videos (link below). This notebook may be confusing otherwise, but watching the lecture series will give you all the background information you need, and it's really good! If you want to see the full implementation, just jump to the bottom of this notebook, or look at the associated python file in this folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LQR](./gym_animation.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Above] Example implementation where random actions are taken at the start, and then LQR controller takes over and stabalizes the cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Pi7l8mMjYVE&ab_channel=SteveBrunton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, lets jump into it. First, let's take a look at the cartpole domain in OpenAI gym. What we can first do is just spin up the environment, take random actions, and see how long it stays up. In this case, I'll set a time out of max number of steps that it can take, and we will stop the simulation when the pole falls over (done is True) or we hit the timeout number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 27\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "timeout = 500\n",
    "\n",
    "observation = env.reset()\n",
    "total_time = 0\n",
    "while True:\n",
    "    #uncomment below if you want to render environment\n",
    "    #env.render()\n",
    "    action = env.action_space.sample() #sample random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_time += 1\n",
    "    if done or total_time == timeout:\n",
    "        break\n",
    "print(\"Total number of steps:\",total_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, when the timeout is 500 and we take random actions, we don't get even close to staying up that long, generally it seems to take a total number of steps less than 100 before falling. Now, the question is, can we do better than this? Rather than use a learning based approach, we will instead use LQR, which requires no learning at all, but does require some assumptions about the dynamics of the problem and our initial setting of the cartpole state. Let's jump into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the state space of the problem. Every observation can be broken into 4 state variables: the position of the cart, the velocity of the cart, the angle of the pole, and the angular velocity of the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation: state. 4D in this case.\n",
    "x, v, theta, v_theta = observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space for this cartpole domain is actually discrete (sending a 0 applies a leftward force on the cart, sending a 1 applies a rightward force). However, LQR will give us a feedback controller that gives continuous actions. We'll deal with that later, but for now, the important thing to know is that we will treat the action space as being 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the dynamics of the system using a system of linear equations (this is what the L stands for in LQR). Unfortunately, the actual dynamics of the cartpole problem are nonlinear. Therefore, what we can do is linearize the cartpole dynamics at the upright unstable equilibrium point, and use that as our approximation to the system dynamics. When the system is fully-controlable, this gurantees that we can derive a linear feedback controller that will drive us to the unstable equilibrium point. However, if the state deviates too far away from the equilibrium point, our controller may not be able to recover. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out the state dynamics of a physical system is generally not-trivial, and I will not get into the details here on how do that and linearize around a certain point. For our purposes, I used the following link, which included a description of the linear dynamics of the cartpole at the unstable upright position: https://metr4202.uqcloud.net/tpl/t8-Week13-pendulum.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for the dynamics requires knowing four constants: the mass of the cart, the mass of the pole, the length of the pole, and the force of gravity. All of these variables are accessible from inside the gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = float(env.masscart)\n",
    "m = float(env.masspole)\n",
    "l = float(env.length)\n",
    "g = float(env.gravity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the equations from the previous link, we can write out the A and B matrix for LQR by plugging in these variables. For this problem, we know that A should be a 4x4 matrix, and B should be a 4x1 matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearization\n",
    "#A = np.identity(4)\n",
    "A = [[0,1,0,0],[0,0,-1*(m*g)/M,0],[0,0,0,1],[0,0,((M+m)*g)/(l*M),0]]\n",
    "A = np.array(A)\n",
    "\n",
    "#B = np.ones((4,1))\n",
    "B = [[0],[1/M],[0],[-1/(l*M)]]\n",
    "B = np.array(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the cost matrices for the state and action, which will define our quadratic cost function (the Q in LQR). For simplicity, we will give equal penalty weighting to all state variables and action dimensions, so we just use the identity matrix. Since the state space is 4d and the action space is 1D, Q and R will be square matrices of the same size respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.identity(4)\n",
    "R = np.identity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got everything we need (a description of the dynamics that is linear in the state and action, and a cost function that is quadratic in the state and action). We can now just use the LQR solver from the control library of python to get the optimal state feedback controller. More information can be found at the following link: https://python-control.readthedocs.io/en/0.8.1/generated/control.lqr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import control\n",
    "\n",
    "K, S, E = control.lqr(A,B,Q,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K is the state feedback gains, S is the solution to the Riccati equation, and E is the eigenvalues of the closed systems. As long as the real parts of the eigenvalues are all negative, then we have a stable controller. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.7559314+0.j         -3.7642922+0.j         -0.8106413+0.49745536j\n",
      " -0.8106413-0.49745536j]\n"
     ]
    }
   ],
   "source": [
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me! All we actually really need is K, because it defines the feedback controller which is linear in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = -1*np.dot(K,observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a 1d continuous action to take, but the cartpole we're working with is discrete action (0 relates to moving leftware, 1 refers to moving rightward). So, we'll just take a simple thresholding based on the sign of the action to decide which discrete action to take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action is negative [[-13.20366438]]\n"
     ]
    }
   ],
   "source": [
    "if action >= 0:\n",
    "    print(\"action is positive\",action)\n",
    "    action = 1\n",
    "else:\n",
    "    print(\"action is negative\",action)\n",
    "    action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's package this all into one function, that will take in the observation, and use LQR to decide the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lqr_policy(observation):\n",
    "    M = float(env.masscart)\n",
    "    m = float(env.masspole)\n",
    "    l = float(env.length)\n",
    "    g = float(env.gravity)\n",
    "    # observation: state. 4D in this case.\n",
    "    x, v, theta, v_theta = observation\n",
    "    # cost function \n",
    "\n",
    "    Q = np.identity(4)\n",
    "    R = np.identity(1)\n",
    "\n",
    "    # linearization\n",
    "    #A = np.identity(4)\n",
    "    A = [[0,1,0,0],[0,0,-1*(m*g)/M,0],[0,0,0,1],[0,0,((M+m)*g)/(l*M),0]]\n",
    "    A = np.array(A)\n",
    "\n",
    "    #B = np.ones((4,1))\n",
    "    B = [[0],[1/M],[0],[-1/(l*M)]]\n",
    "    B = np.array(B)\n",
    "\n",
    "    #K (2-d array) – State feedback gains: ???\n",
    "    #S (2-d array) – Solution to Riccati equation\n",
    "    #E (1-d array) – Eigenvalues of the closed loop system\n",
    "    #print(\"A:\",A.shape)\n",
    "    #print(\"B:\",B.shape)\n",
    "    #print(\"Q:\",Q.shape)\n",
    "    #print(\"R:\",R.shape)\n",
    "    K, S, E = control.lqr(A,B,Q,R)\n",
    "    #print(\"K:\",K)\n",
    "    #print(\"S:\",S)\n",
    "    #print(\"E:\",E)\n",
    "    #print(\"Observation:\",observation)\n",
    "    action = -1*np.dot(K,observation)\n",
    "    if action >= 0:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not changing A,B,Q,R between time steps, we don't need to continually recompute K each time step, but instead compute K once at the start and use it to define our linear feedback controller, but it's pretty fast in this domain so it's not a big issue to do the computation over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what happens when we substitue in LQR for the random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 500\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "timeout = 500\n",
    "\n",
    "observation = env.reset()\n",
    "total_time = 0\n",
    "while True:\n",
    "    #uncomment below if you want to render environment\n",
    "    env.render()\n",
    "    action = lqr_policy(observation) #sample random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_time += 1\n",
    "    if done or total_time == timeout:\n",
    "        break\n",
    "print(\"Total number of steps:\",total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're consistently hitting the max number of time steps allowed by the timeout, and if you render the environment, you can watch the cartpole stabalize! It's important to remember that we linearized around the unstable, upright equilibrium point, which means that as our state diverges from that equilibrium, our approximation to the dynamics gets worse and worse, making our controller worse and worse. When the cartpole starts about upright, like when we do reset, this is no problem, but once the cartpole falls over, it can't recover. As long as our approximation to the dynamics is reasonable, our controller should be able to recover. For example, let's say that for the first 10 steps, we act according to the random policy, and then start doing LQR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 500\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import control\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "timeout = 500\n",
    "\n",
    "observation = env.reset()\n",
    "total_time = 0\n",
    "while True:\n",
    "    #uncomment below if you want to render environment\n",
    "    #env.render()\n",
    "    action = env.action_space.sample() if total_time <= 10 else lqr_policy(observation) #sample random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_time += 1\n",
    "    if done or total_time == timeout:\n",
    "        break\n",
    "print(\"Total number of steps:\",total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, even when we take 10 random actions, it seems to recover and get back to the stable point. But if we take too many random actions, like 50, recovery becomes highly unlikely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps: 11\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import control\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "timeout = 500\n",
    "\n",
    "observation = env.reset()\n",
    "total_time = 0\n",
    "while True:\n",
    "    #uncomment below if you want to render environment\n",
    "    #env.render()\n",
    "    action = env.action_space.sample() if total_time <= 50 else lqr_policy(observation) #sample random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_time += 1\n",
    "    if done or total_time == timeout:\n",
    "        break\n",
    "print(\"Total number of steps:\",total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with Q and R to set different costs for the state and action, which will change how aggresive the controller is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
